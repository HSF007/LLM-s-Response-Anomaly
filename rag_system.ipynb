{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df363ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import string\n",
    "import logging\n",
    "import hashlib\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import PyPDF2\n",
    "import docx\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6c7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(filename='logs.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd0db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = os.path.join(os.getcwd(), 'config.yaml')\n",
    "config = None\n",
    "with open(params_path) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecddb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DOCUMENTS_FOLDER = \"./test\"  # documents folder\n",
    "GROQ_API_KEY = config['api']['groq']  # Groq API key\n",
    "CHROMA_DB_PATH = \"./database\"  # ChromaDB storage path\n",
    "\n",
    "# Create documents folder if it doesn't exist\n",
    "os.makedirs(DOCUMENTS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e71e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentHandler(FileSystemEventHandler):\n",
    "    \"\"\"File system event handler for document monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            filepath = Path(event.src_path)\n",
    "            if filepath.suffix.lower() in self.rag_system.supported_extensions:\n",
    "                logging.info(f\"New document detected: {filepath.name}\")\n",
    "                # Add a small delay to ensure file is fully written\n",
    "                time.sleep(2)\n",
    "                self.rag_system.process_document(filepath)\n",
    "    \n",
    "    def on_modified(self, event):\n",
    "        if not event.is_directory:\n",
    "            filepath = Path(event.src_path)\n",
    "            if filepath.suffix.lower() in self.rag_system.supported_extensions:\n",
    "                logging.info(f\"Document modified: {filepath.name}\")\n",
    "                # Add a small delay to ensure file is fully written\n",
    "                time.sleep(2)\n",
    "                self.rag_system.process_document(filepath)\n",
    "    \n",
    "    def on_deleted(self, event):\n",
    "        if not event.is_directory:\n",
    "            filepath = Path(event.src_path)\n",
    "            if filepath.suffix.lower() in self.rag_system.supported_extensions:\n",
    "                logging.info(f\"Document deleted: {filepath.name}\")\n",
    "                self.rag_system.delete_document(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677305ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    def __init__(self, \n",
    "                 documents_folder: str,\n",
    "                 groq_api_key: str,\n",
    "                 db_path: str = \"./chroma_db\",\n",
    "                 collection_name: str = \"document_chunks\",\n",
    "                 groq_model: str = \"llama3-8b-8192\",\n",
    "                 chunk_size: int = 512,\n",
    "                 chunk_overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system with ChromaDB\n",
    "        \n",
    "        Args:\n",
    "            documents_folder: Path to folder containing documents\n",
    "            groq_api_key: Groq API key\n",
    "            db_path: Path to ChromaDB database directory\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "        self.documents_folder = Path(documents_folder)\n",
    "        self.groq_client = Groq(api_key=groq_api_key)\n",
    "        self.db_path = db_path\n",
    "        self.collection_name = collection_name\n",
    "        self.groq_model = groq_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Initialize ChromaDB\n",
    "        self.init_chromadb()\n",
    "        \n",
    "        # File monitoring\n",
    "        self.observer = None\n",
    "        self.event_handler = DocumentHandler(self)\n",
    "        \n",
    "        # Supported file extensions\n",
    "        self.supported_extensions = {'.txt', '.pdf', '.docx', '.md'}\n",
    "        \n",
    "        # Document tracking (to check if files have been modified)\n",
    "        self.document_hashes = self.load_document_hashes()\n",
    "        \n",
    "        logging.info(\"RAG System initialized with ChromaDB\")\n",
    "\n",
    "    def init_chromadb(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create ChromaDB client with persistent storage\n",
    "            self.chroma_client = chromadb.PersistentClient(\n",
    "                path=self.db_path,\n",
    "                settings=Settings(\n",
    "                    anonymized_telemetry=False,\n",
    "                    allow_reset=True\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.chroma_client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Document chunks for RAG system\"}\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"ChromaDB initialized with collection: {self.collection_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_document_hashes(self) -> Dict[str, str]:\n",
    "        \"\"\"Load document hashes from metadata file\"\"\"\n",
    "        hash_file = Path(self.db_path) / \"document_hashes.json\"\n",
    "        if hash_file.exists():\n",
    "            try:\n",
    "                with open(hash_file, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error loading document hashes: {e}\")\n",
    "        return {}\n",
    "\n",
    "    def save_document_hashes(self):\n",
    "        \"\"\"Save document hashes to metadata file\"\"\"\n",
    "        hash_file = Path(self.db_path) / \"document_hashes.json\"\n",
    "        os.makedirs(Path(self.db_path), exist_ok=True)\n",
    "        try:\n",
    "            with open(hash_file, 'w') as f:\n",
    "                json.dump(self.document_hashes, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving document hashes: {e}\")\n",
    "\n",
    "    def get_file_hash(self, filepath: Path) -> str:\n",
    "        \"\"\"Generate MD5 hash of file content\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "\n",
    "    def extract_text_from_file(self, filepath: Path) -> str:\n",
    "        \"\"\"Extract text from various file formats\"\"\"\n",
    "        try:\n",
    "            if filepath.suffix.lower() == '.pdf':\n",
    "                return self._extract_pdf_text(filepath)\n",
    "            elif filepath.suffix.lower() == '.docx':\n",
    "                return self._extract_docx_text(filepath)\n",
    "            elif filepath.suffix.lower() in ['.txt', '.md']:\n",
    "                with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                    return file.read()\n",
    "            else:\n",
    "                logging.warning(f\"Unsupported file format: {filepath.suffix}\")\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting text from {filepath}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_pdf_text(self, filepath: Path) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(filepath, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading PDF {filepath}: {e}\")\n",
    "        return text\n",
    "\n",
    "    def _extract_docx_text(self, filepath: Path) -> str:\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(filepath)\n",
    "            text = \"\"\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading DOCX {filepath}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def create_chunks(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        if not text.strip():\n",
    "            return []\n",
    "        \n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk_words = words[i:i + self.chunk_size]\n",
    "            chunk_text = ' '.join(chunk_words)\n",
    "            if chunk_text.strip():  # Only add non-empty chunks\n",
    "                chunks.append(chunk_text)\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def create_embeddings(self, chunks: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Create embeddings for text chunks\"\"\"\n",
    "        if not chunks:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            embeddings = self.embedding_model.encode(chunks)\n",
    "            # Convert numpy arrays to lists for ChromaDB compatibility\n",
    "            return [embedding.tolist() for embedding in embeddings]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error creating embeddings: {e}\")\n",
    "            return []\n",
    "\n",
    "    def is_document_updated(self, filepath: Path) -> bool:\n",
    "        \"\"\"Check if document has been updated since last processing\"\"\"\n",
    "        current_hash = self.get_file_hash(filepath)\n",
    "        filename = str(filepath)\n",
    "        \n",
    "        # Check if file hash has changed\n",
    "        if filename not in self.document_hashes:\n",
    "            return True\n",
    "        \n",
    "        return self.document_hashes[filename] != current_hash\n",
    "\n",
    "    def remove_document_from_db(self, filepath: Path):\n",
    "        \"\"\"Remove document chunks from ChromaDB\"\"\"\n",
    "        try:\n",
    "            filename = str(filepath)\n",
    "            \n",
    "            # Get all documents with this filename\n",
    "            results = self.collection.get(\n",
    "                where={\"filename\": filename}\n",
    "            )\n",
    "            \n",
    "            if results['ids']:\n",
    "                # Delete all chunks for this document\n",
    "                self.collection.delete(ids=results['ids'])\n",
    "                logging.info(f\"Removed {len(results['ids'])} chunks for {filepath.name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing document from ChromaDB: {e}\")\n",
    "\n",
    "    def process_document(self, filepath: Path):\n",
    "        \"\"\"Process a single document: extract text, create chunks and embeddings\"\"\"\n",
    "        if filepath.suffix.lower() not in self.supported_extensions:\n",
    "            logging.warning(f\"Skipping unsupported file: {filepath}\")\n",
    "            return\n",
    "        \n",
    "        if not self.is_document_updated(filepath):\n",
    "            logging.info(f\"Document {filepath.name} already up to date\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Processing document: {filepath.name}\")\n",
    "        \n",
    "        # Remove existing chunks for this document\n",
    "        self.remove_document_from_db(filepath)\n",
    "        \n",
    "        # Extract text\n",
    "        text = self.extract_text_from_file(filepath)\n",
    "        if not text.strip():\n",
    "            logging.warning(f\"No text extracted from {filepath}\")\n",
    "            return\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = self.create_chunks(text)\n",
    "        if not chunks:\n",
    "            logging.warning(f\"No chunks created from {filepath}\")\n",
    "            return\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = self.create_embeddings(chunks)\n",
    "        if not embeddings:\n",
    "            logging.error(f\"Failed to create embeddings for {filepath}\")\n",
    "            return\n",
    "        \n",
    "        # Save to ChromaDB\n",
    "        self.save_document_to_db(filepath, chunks, embeddings)\n",
    "        \n",
    "        # Update document hash\n",
    "        self.document_hashes[str(filepath)] = self.get_file_hash(filepath)\n",
    "        self.save_document_hashes()\n",
    "        \n",
    "        logging.info(f\"Successfully processed {filepath.name} with {len(chunks)} chunks\")\n",
    "\n",
    "    def save_document_to_db(self, filepath: Path, chunks: List[str], embeddings: List[List[float]]):\n",
    "        \"\"\"Save document chunks and embeddings to ChromaDB\"\"\"\n",
    "        try:\n",
    "            filename = str(filepath)\n",
    "            file_hash = self.get_file_hash(filepath)\n",
    "            last_modified = filepath.stat().st_mtime\n",
    "            \n",
    "            # Prepare data for ChromaDB\n",
    "            ids = []\n",
    "            documents = []\n",
    "            metadatas = []\n",
    "            chunk_embeddings = []\n",
    "            \n",
    "            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "                chunk_id = f\"{filename}_{file_hash}_{i}\"\n",
    "                \n",
    "                ids.append(chunk_id)\n",
    "                documents.append(chunk)\n",
    "                metadatas.append({\n",
    "                    \"filename\": filename,\n",
    "                    \"file_hash\": file_hash,\n",
    "                    \"last_modified\": last_modified,\n",
    "                    \"chunk_index\": i,\n",
    "                    \"file_extension\": filepath.suffix.lower()\n",
    "                })\n",
    "                chunk_embeddings.append(embedding)\n",
    "            \n",
    "            # Add to ChromaDB collection\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                documents=documents,\n",
    "                embeddings=chunk_embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Saved {len(chunks)} chunks to ChromaDB for {filepath.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving document to ChromaDB: {e}\")\n",
    "            raise\n",
    "\n",
    "    def scan_documents_folder(self):\n",
    "        \"\"\"Scan the documents folder and process all files\"\"\"\n",
    "        if not self.documents_folder.exists():\n",
    "            logging.error(f\"Documents folder does not exist: {self.documents_folder}\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Scanning documents folder: {self.documents_folder}\")\n",
    "        \n",
    "        for filepath in self.documents_folder.rglob('*'):\n",
    "            if filepath.is_file() and filepath.suffix.lower() in self.supported_extensions:\n",
    "                self.process_document(filepath)\n",
    "\n",
    "    def search_similar_chunks(self, query: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:\n",
    "        \"\"\"Search for similar chunks based on query using ChromaDB\"\"\"\n",
    "        try:\n",
    "            # Create query embedding\n",
    "            query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
    "            \n",
    "            # Search in ChromaDB\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=min(top_k, self.get_total_chunks()),\n",
    "                include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "            )\n",
    "            \n",
    "            if not results['documents'][0]:\n",
    "                logging.warning(\"No chunks found in ChromaDB\")\n",
    "                return []\n",
    "            \n",
    "            # Convert distances to similarities (ChromaDB returns squared euclidean distances)\n",
    "            # Similarity = 1 / (1 + distance)\n",
    "            similarities = []\n",
    "            for doc, metadata, distance in zip(results['documents'][0], \n",
    "                                             results['metadatas'][0], \n",
    "                                             results['distances'][0]):\n",
    "                similarity = 1 / (1 + distance)\n",
    "                similarities.append((doc, similarity, metadata))\n",
    "            \n",
    "            return similarities\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching similar chunks: {e}\")\n",
    "            return []\n",
    "\n",
    "    def generate_answer(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"Generate answer using Groq API with retrieved context\"\"\"\n",
    "        try:\n",
    "            # Prepare context\n",
    "            context = \"\\n\\n\".join(context_chunks)\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = f\"\"\"Answer the question based only on the following context. If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: Answer the question based on the above context: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "            \n",
    "            # Call Groq API\n",
    "            response = self.groq_client.chat.completions.create(\n",
    "                model=self.groq_model,  # You can change this to other Groq models like \"mixtral-8x7b-32768\"\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1024,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating answer: {e}\")\n",
    "            return f\"Error generating answer: {str(e)}\"\n",
    "\n",
    "    def answer_question(self, query: str, top_k: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"Answer a question using RAG with ChromaDB\"\"\"\n",
    "        logging.info(f\"Answering question: {query}\")\n",
    "        \n",
    "        # Search for similar chunks\n",
    "        similar_chunks = self.search_similar_chunks(query, top_k)\n",
    "        \n",
    "        if not similar_chunks:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant information in the documents to answer your question.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0,\n",
    "                \"source_files\": []\n",
    "            }\n",
    "        \n",
    "        # Extract context chunks and calculate average confidence\n",
    "        context_chunks = [chunk[0] for chunk in similar_chunks]\n",
    "        confidences = [chunk[1] for chunk in similar_chunks]\n",
    "        metadatas = [chunk[2] for chunk in similar_chunks]\n",
    "        avg_confidence = sum(confidences) / len(confidences)\n",
    "        \n",
    "        # Extract unique source files\n",
    "        source_files = list(set([meta['filename'] for meta in metadatas]))\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(query, context_chunks)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [(chunk[0], chunk[1]) for chunk in similar_chunks],\n",
    "            \"confidence\": avg_confidence,\n",
    "            \"source_files\": source_files,\n",
    "            \"metadata\": metadatas\n",
    "        }\n",
    "\n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start monitoring the documents folder for changes\"\"\"\n",
    "        if not self.documents_folder.exists():\n",
    "            logging.error(f\"Cannot monitor non-existent folder: {self.documents_folder}\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Starting to monitor folder: {self.documents_folder}\")\n",
    "        \n",
    "        self.observer = Observer()\n",
    "        self.observer.schedule(self.event_handler, str(self.documents_folder), recursive=True)\n",
    "        self.observer.start()\n",
    "        \n",
    "        logging.info(\"File monitoring started\")\n",
    "\n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop monitoring the documents folder\"\"\"\n",
    "        if self.observer and self.observer.is_alive():\n",
    "            self.observer.stop()\n",
    "            self.observer.join()\n",
    "            logging.info(\"File monitoring stopped\")\n",
    "\n",
    "    def get_total_chunks(self) -> int:\n",
    "        \"\"\"Get total number of chunks in the database\"\"\"\n",
    "        try:\n",
    "            count = self.collection.count()\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting chunk count: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def get_database_stats(self) -> Dict[str, any]:\n",
    "        \"\"\"Get statistics about the ChromaDB database\"\"\"\n",
    "        try:\n",
    "            total_chunks = self.get_total_chunks()\n",
    "            \n",
    "            # Get unique documents\n",
    "            if total_chunks > 0:\n",
    "                results = self.collection.get(include=[\"metadatas\"])\n",
    "                unique_files = set()\n",
    "                file_types = {}\n",
    "                \n",
    "                for metadata in results['metadatas']:\n",
    "                    filename = metadata['filename']\n",
    "                    file_ext = metadata.get('file_extension', 'unknown')\n",
    "                    \n",
    "                    unique_files.add(filename)\n",
    "                    file_types[file_ext] = file_types.get(file_ext, 0) + 1\n",
    "                \n",
    "                return {\n",
    "                    \"total_chunks\": total_chunks,\n",
    "                    \"unique_documents\": len(unique_files),\n",
    "                    \"file_types\": file_types,\n",
    "                    \"documents\": list(unique_files)\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"total_chunks\": 0,\n",
    "                    \"unique_documents\": 0,\n",
    "                    \"file_types\": {},\n",
    "                    \"documents\": []\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting database stats: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def search_documents_by_filename(self, filename_pattern: str) -> List[str]:\n",
    "        \"\"\"Search for documents by filename pattern\"\"\"\n",
    "        try:\n",
    "            results = self.collection.get(\n",
    "                where={\"filename\": {\"$contains\": filename_pattern}},\n",
    "                include=[\"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            unique_files = set()\n",
    "            for metadata in results['metadatas']:\n",
    "                unique_files.add(metadata['filename'])\n",
    "            \n",
    "            return list(unique_files)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error searching documents: {e}\")\n",
    "            return []\n",
    "\n",
    "    def delete_document(self, filepath: Path):\n",
    "        \"\"\"Delete a document and all its chunks from ChromaDB\"\"\"\n",
    "        try:\n",
    "            filename = str(filepath)\n",
    "            \n",
    "            # Get all chunks for this document\n",
    "            results = self.collection.get(\n",
    "                where={\"filename\": filename}\n",
    "            )\n",
    "            \n",
    "            if results['ids']:\n",
    "                # Delete all chunks for this document\n",
    "                self.collection.delete(ids=results['ids'])\n",
    "                logging.info(f\"Deleted {len(results['ids'])} chunks for {filepath.name}\")\n",
    "                \n",
    "                # Remove from document hashes\n",
    "                if filename in self.document_hashes:\n",
    "                    del self.document_hashes[filename]\n",
    "                    self.save_document_hashes()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error deleting document: {e}\")\n",
    "\n",
    "    def reset_database(self):\n",
    "        \"\"\"Reset the entire ChromaDB database\"\"\"\n",
    "        try:\n",
    "            self.chroma_client.delete_collection(self.collection_name)\n",
    "            self.collection = self.chroma_client.create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"Document chunks for RAG system\"}\n",
    "            )\n",
    "            self.document_hashes = {}\n",
    "            self.save_document_hashes()\n",
    "            logging.info(\"Database reset successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error resetting database: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e989882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "llm_models = None\n",
    "file_path = os.path.join(os.getcwd(), 'llm_models.txt')\n",
    "with open(file_path, 'r') as f:\n",
    "    llm_models = [line.strip() for line in f if line.strip()]\n",
    "print(len(llm_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce430a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self, language = 'english'):\n",
    "        self.language = language\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        # Try to load spaCy model for advanced processing\n",
    "        self.nlp = None\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except OSError:\n",
    "            print(\"Install with: python -m spacy download en_core_web_sm\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        # Remove or replace special characters and symbols\n",
    "        text = re.sub(r'[^\\w\\s\\.\\!\\?\\,\\:\\;\\-\\'\\\"]', ' ', text)\n",
    "        \n",
    "        # Fix common encoding issues\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        \n",
    "        # Normalize unicode characters\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def normalize_text(self, text, \n",
    "                      lowercase = True,\n",
    "                      remove_punctuation = False,\n",
    "                      remove_numbers = False,\n",
    "                      expand_contractions = True):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Expand contractions\n",
    "        if expand_contractions:\n",
    "            text = self._expand_contractions(text)\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Remove numbers\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        if remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Clean up extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _expand_contractions(self, text):\n",
    "        contractions = {\n",
    "            \"ain't\": \"are not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
    "            \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "            \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
    "            \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "            \"it'll\": \"it will\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "            \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "            \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
    "            \"we've\": \"we have\", \"weren't\": \"were not\", \"what's\": \"what is\",\n",
    "            \"where's\": \"where is\", \"who's\": \"who is\", \"won't\": \"will not\",\n",
    "            \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\",\n",
    "            \"you're\": \"you are\", \"you've\": \"you have\"\n",
    "        }\n",
    "        \n",
    "        for contraction, expansion in contractions.items():\n",
    "            text = re.sub(r'\\b' + contraction + r'\\b', expansion, text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text, custom_stopwords = None):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        stop_words = self.stop_words.copy()\n",
    "        \n",
    "        if custom_stopwords:\n",
    "            stop_words.update(custom_stopwords)\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        \n",
    "        return ' '.join(filtered_tokens)\n",
    "    \n",
    "    def lemmatize_text(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatized = [self.lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "        \n",
    "        return ' '.join(lemmatized)\n",
    "    \n",
    "    def stem_text(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        stemmed = [self.stemmer.stem(token.lower()) for token in tokens]\n",
    "        \n",
    "        return ' '.join(stemmed)\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        entities = defaultdict(list)\n",
    "        \n",
    "        if self.nlp is None:\n",
    "            return dict(entities)\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "        \n",
    "        return dict(entities)\n",
    "    \n",
    "    def correct_spelling(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            blob = TextBlob(text)\n",
    "            return str(blob.correct())\n",
    "        except:\n",
    "            return text\n",
    "    \n",
    "    def segment_sentences(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        return sent_tokenize(text)\n",
    "    \n",
    "    def preprocess(self, text, \n",
    "                   clean = True,\n",
    "                   normalize = True,\n",
    "                   remove_stopwords = False,\n",
    "                   lemmatize = False,\n",
    "                   stem = False,\n",
    "                   correct_spelling = False,\n",
    "                   custom_stopwords = None,\n",
    "                   preserve_entities = False):\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        original_entities = {}\n",
    "        if preserve_entities:\n",
    "            original_entities = self.extract_entities(text)\n",
    "        \n",
    "        # Step 1: Clean text\n",
    "        if clean:\n",
    "            text = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Spelling correction (before other processing)\n",
    "        if correct_spelling:\n",
    "            text = self.correct_spelling(text)\n",
    "        \n",
    "        # Step 3: Normalize text\n",
    "        if normalize:\n",
    "            text = self.normalize_text(text, \n",
    "                                     lowercase=True,\n",
    "                                     remove_punctuation=False,\n",
    "                                     remove_numbers=False,\n",
    "                                     expand_contractions=True)\n",
    "        \n",
    "        # Step 4: Remove stopwords\n",
    "        if remove_stopwords:\n",
    "            text = self.remove_stopwords(text, custom_stopwords)\n",
    "        \n",
    "        # Step 5: Lemmatize or stem (mutually exclusive)\n",
    "        if lemmatize and not stem:\n",
    "            text = self.lemmatize_text(text)\n",
    "        elif stem and not lemmatize:\n",
    "            text = self.stem_text(text)\n",
    "        elif lemmatize and stem:\n",
    "            print(\"Warning: Both lemmatize and stem are True. Using lemmatization only.\")\n",
    "            text = self.lemmatize_text(text)\n",
    "        \n",
    "        # Step 6: Final cleanup\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7904f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLITextSimilarity:\n",
    "    def __init__(self, model_name = \"microsoft/deberta-v2-xlarge-mnli\", \n",
    "                 enable_preprocessing = True, \n",
    "                 preprocessing_config = None):\n",
    "        self.model_name = model_name\n",
    "        self.enable_preprocessing = enable_preprocessing\n",
    "        \n",
    "        # Default preprocessing configuration\n",
    "        self.preprocessing_config = {\n",
    "            'clean': True,\n",
    "            'normalize': True,\n",
    "            'remove_stopwords': False,\n",
    "            'lemmatize': False,\n",
    "            'stem': False,\n",
    "            'correct_spelling': False,\n",
    "            'preserve_entities': False\n",
    "        }\n",
    "        \n",
    "        # Update with user config\n",
    "        if preprocessing_config:\n",
    "            self.preprocessing_config.update(preprocessing_config)\n",
    "        \n",
    "        # Initialize preprocessor\n",
    "        if self.enable_preprocessing:\n",
    "            self.preprocessor = TextPreprocessor()\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Label mapping for MNLI models\n",
    "        self.label_mapping = {\n",
    "            'CONTRADICTION': 0,\n",
    "            'NEUTRAL': 1, \n",
    "            'ENTAILMENT': 2\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text, custom_config = None):\n",
    "        if not self.enable_preprocessing:\n",
    "            return text\n",
    "        \n",
    "        config = self.preprocessing_config.copy()\n",
    "        if custom_config:\n",
    "            config.update(custom_config)\n",
    "        \n",
    "        return self.preprocessor.preprocess(text, **config)\n",
    "        \n",
    "    def get_nli_score(self, premise, hypothesis, \n",
    "                     preprocess = None):\n",
    "        # Apply preprocessing if enabled\n",
    "        if preprocess is None:\n",
    "            preprocess = self.enable_preprocessing\n",
    "        \n",
    "        if preprocess and self.enable_preprocessing:\n",
    "            premise = self.preprocess_text(premise)\n",
    "            hypothesis = self.preprocess_text(hypothesis)\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(premise, hypothesis, \n",
    "                              return_tensors=\"pt\", \n",
    "                              truncation=True, \n",
    "                              max_length=512,\n",
    "                              padding=True)\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "            \n",
    "        # Convert to numpy and extract probabilities\n",
    "        probs = probabilities.squeeze().numpy()\n",
    "        \n",
    "        return {\n",
    "            'contradiction': float(probs[0]),\n",
    "            'neutral': float(probs[1]),\n",
    "            'entailment': float(probs[2])\n",
    "        }\n",
    "    \n",
    "    def bidirectional_similarity(self, text1, text2, \n",
    "                                preprocess = None):\n",
    "        # Get NLI scores in both directions\n",
    "        scores_1_to_2 = self.get_nli_score(text1, text2, preprocess)\n",
    "        scores_2_to_1 = self.get_nli_score(text2, text1, preprocess)\n",
    "        \n",
    "        # Average the entailment probabilities\n",
    "        similarity = (scores_1_to_2['entailment'] + scores_2_to_1['entailment']) / 2\n",
    "        \n",
    "        return similarity\n",
    "    \n",
    "    def compute_similarity_matrix(self, responses, preprocess = None):\n",
    "        n = len(responses)\n",
    "        similarity_matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i, j] = 1.0  # Self-similarity is 1\n",
    "                else:\n",
    "                    scores_1_to_2 = self.get_nli_score(responses[i], responses[j], preprocess)\n",
    "                    scores_2_to_1 = self.get_nli_score(responses[j], responses[i], preprocess)\n",
    "                    similarity_matrix[i, j] = similarity_matrix[j, i] = (scores_1_to_2['entailment'] + scores_2_to_1['entailment']) / 2\n",
    "        \n",
    "        return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyQuantifier:\n",
    "    def __init__(self):\n",
    "        self.word_sets = []\n",
    "        self.eigenvalues = None\n",
    "    \n",
    "    def get_similarity_matrix(self, similarity_matrix):\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def get_eigenvalues(self):\n",
    "        return self.eigenvalues\n",
    "\n",
    "    def preprocess_sentence(self, sentence):\n",
    "        words = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        return set(words)\n",
    "    \n",
    "    def jaccard_similarity(self, response1, response2):\n",
    "        intersection = len(response1.intersection(response2))\n",
    "        union = len(response1.union(response2))\n",
    "        \n",
    "        # Handle empty sets\n",
    "        if union == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return intersection / union\n",
    "    \n",
    "    def compute_similarity_matrix(self, responses):\n",
    "        n = len(responses)\n",
    "        similarity_matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Preprocess all responses\n",
    "        self.word_sets = [self.preprocess_sentence(sentence) for sentence in responses]\n",
    "        \n",
    "        # Calculate pairwise similarities\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                if i == j:\n",
    "                    similarity_matrix[i, j] = 1.0  # Self-similarity is 1\n",
    "                else:\n",
    "                    sim = self.jaccard_similarity(self.word_sets[i], self.word_sets[j])\n",
    "                    similarity_matrix[i, j] = similarity_matrix[j, i] = sim\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def num_semantic_sets(self, similarity_matrix, threshold = 0.5):\n",
    "        n = len(similarity_matrix)\n",
    "        if n <= 1:\n",
    "            return n\n",
    "        \n",
    "        groups = list(range(n))  # Initially each response is its own group\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                # precomputed similarity from matrix\n",
    "                jaccard = similarity_matrix[i, j]\n",
    "                nli_scores = {\n",
    "                        'entailment': jaccard,\n",
    "                        'contradiction': 1 - jaccard\n",
    "                }\n",
    "                \n",
    "                # Check bidirectional entailment\n",
    "                if (nli_scores['entailment'] > threshold and \n",
    "                    nli_scores['entailment'] > nli_scores['contradiction']):\n",
    "                    \n",
    "                    min_group = min(groups[i], groups[j])\n",
    "                    max_group = max(groups[i], groups[j])\n",
    "                    groups = [min_group if g == max_group else g for g in groups]\n",
    "        \n",
    "        return len(set(groups))\n",
    "    \n",
    "    def eigenvalue_uncertainty(self, similarity_matrix):\n",
    "        n = similarity_matrix.shape[0]\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compute degree matrix\n",
    "        degree_matrix = np.diag(similarity_matrix.sum(axis=1))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        degree_sqrt_inv = np.zeros_like(degree_matrix)\n",
    "        for i in range(n):\n",
    "            if degree_matrix[i, i] > 1e-10:\n",
    "                degree_sqrt_inv[i, i] = 1.0 / np.sqrt(degree_matrix[i, i])\n",
    "        \n",
    "        # Compute Laplacian\n",
    "        laplacian = np.eye(n) - degree_sqrt_inv @ similarity_matrix @ degree_sqrt_inv\n",
    "        \n",
    "        # Compute eigenvalues\n",
    "        self.eigenvalues = np.linalg.eigvals(laplacian)\n",
    "        self.eigenvalues = np.real(self.eigenvalues)\n",
    "        self.eigenvalues = np.sort(self.eigenvalues)\n",
    "        \n",
    "        # Sum of (1 - lambda_k) for lambda_k <= 1\n",
    "        uncertainty = sum(max(0, 1 - lam) for lam in self.eigenvalues if lam <= 1)\n",
    "        return uncertainty\n",
    "    \n",
    "    def degree_based_measures(self, similarity_matrix):\n",
    "        n = similarity_matrix.shape[0]\n",
    "        \n",
    "        # Compute degree for each node\n",
    "        degrees = similarity_matrix.sum(axis=1)\n",
    "        \n",
    "        # Normalize degrees\n",
    "        max_degree = n - 1 if n > 1 else 1\n",
    "        normalized_degrees = degrees / max_degree\n",
    "        \n",
    "        # Uncertainty: average distance from maximum connectivity\n",
    "        uncertainty = np.mean(1 - normalized_degrees)\n",
    "        \n",
    "        # Confidence: individual degrees (higher degree = higher confidence)\n",
    "        confidence_scores = normalized_degrees\n",
    "        \n",
    "        return uncertainty, confidence_scores\n",
    "    \n",
    "    def eccentricity_measures(self, similarity_matrix, k = 2):\n",
    "        n = similarity_matrix.shape[0]\n",
    "        if n <= 1:\n",
    "            return 0.0, np.array([1.0] * n)\n",
    "        # Compute degree matrix\n",
    "        degree_matrix = np.diag(similarity_matrix.sum(axis=1))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        degree_sqrt_inv = np.zeros_like(degree_matrix)\n",
    "        for i in range(n):\n",
    "            if degree_matrix[i, i] > 1e-10:\n",
    "                degree_sqrt_inv[i, i] = 1.0 / np.sqrt(degree_matrix[i, i])\n",
    "        \n",
    "        # Compute normalized Laplacian\n",
    "        laplacian = np.eye(n) - degree_sqrt_inv @ similarity_matrix @ degree_sqrt_inv\n",
    "        \n",
    "        # Get k smallest eigenvectors\n",
    "        k = min(k, n-1) if n > 1 else 1\n",
    "        eigenvals, eigenvecs = eigh(laplacian)\n",
    "        \n",
    "        # Use first k eigenvectors for embedding\n",
    "        embedding = eigenvecs[:, :k]\n",
    "        \n",
    "        # Center the embeddings\n",
    "        centroid = np.mean(embedding, axis=0)\n",
    "        centered_embedding = embedding - centroid\n",
    "        \n",
    "        # Calculate distances from center\n",
    "        distances = np.linalg.norm(centered_embedding, axis=1)\n",
    "        \n",
    "        # Uncertainty: average distance from center\n",
    "        uncertainty = np.mean(distances)\n",
    "        \n",
    "        # Confidence: negative distance (closer to center = higher confidence)\n",
    "        max_dist = np.max(distances) if np.max(distances) > 0 else 1\n",
    "        confidence_scores = 1 - (distances / max_dist)\n",
    "        \n",
    "        return uncertainty, confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_models = None\n",
    "df = pd.DataFrame(columns=['model', 'question_level', 'similarity_method', 'eigenvalue_uncertanity', 'degree_uncertanity', 'eccentricity_uncertanity', 'avg_degree_confidence', 'avg_eccentirc_confidence', 'responce'])\n",
    "\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), 'llm_models.txt')\n",
    "with open(file_path, 'r') as f:\n",
    "    llm_models = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "\n",
    "for ind, models in enumerate(llm_models):\n",
    "    # Initialize RAG system\n",
    "    rag = RAGSystem(\n",
    "        documents_folder=DOCUMENTS_FOLDER,\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        db_path=CHROMA_DB_PATH,\n",
    "        collection_name=\"document_chunks\",\n",
    "        groq_model = \"llama3-8b-8192\",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    # Initial scan of documents folder\n",
    "    rag.scan_documents_folder()\n",
    "\n",
    "    # Start monitoring for new/modified documents\n",
    "    rag.start_monitoring()\n",
    "\n",
    "    questions = ['Who is sovereign in India?',\n",
    "                 'Which Fundamental Rights is guaranteed only to Indian citizens and not to foreigners?',\n",
    "                 'Is India a true federation or quasi-federal state?'\n",
    "    ]\n",
    "    responses = []\n",
    "    for level, question in enumerate(questions, 1):\n",
    "        for i in range(30):\n",
    "            result = rag.answer_question(question, top_k=5)\n",
    "            responses.append(result['answer'])\n",
    "        \n",
    "        nli_sim_model_1 = NLITextSimilarity(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "        sim_matrix = nli_sim_model_1.compute_similarity_matrix(responses=responses)\n",
    "\n",
    "        uq = UncertaintyQuantifier()\n",
    "        un_egiv = uq.eigenvalue_uncertainty(sim_matrix)\n",
    "        deg_un, deg_conf = uq.degree_based_measures(sim_matrix)\n",
    "        ecc_un, ecc_conf = uq.eccentricity_measures(sim_matrix, k=4)\n",
    "        save_response = np.argmax(ecc_conf)\n",
    "        \n",
    "        df.loc[ind] = [models, level, 'NLI', un_egiv, deg_un, ecc_un, np.mean(deg_conf), np.mean(ecc_conf), responses[save_response]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5512121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('llm_models_uncertanity.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
